---
layout: post
title:  operation system
categories: operation_system
---
# 1 操作系统的历史

* 多任务处理是操作系统诞生的驱动因素，或者说操作系统解决的问题是如何高效地进行多任务处理

  如果单任务独占，那么不需要操作系统

  发展的进程：

  单任务独占 --> 批处理 --> 多任务处理

* 硬件上

  * 处理器的进步使得计算效率大大提升
  * 内存的增大使得计算机同时装载多个程序成为可能

* 对 cpu 进行分时复用的关键技术是中断

* 虚拟化的目的是提供优秀的 api，让程序的功能得以实现，并且在使用时好像是在独占设备资源

* three easy pieces

  * 虚拟化

    进程，虚拟内存，设备抽象

  * 并发

    cpu 的分时复用，进程/线程间的通信

  * 持久化

    文件系统



# 2 应用眼中的操作系统

* coreutils

  体积更小的替代品，busybox

* ubuntu packages 支持文件名检索

  当系统缺少某些东西时，这是一个非常好的检索入口

* elf

  executable linkable format，可执行的可链接的格式

  * xxd，反汇编命令，可以用来分析 elf 格式的文件
  * readelf，可以用来查看 elf 格式的文件的信息



# 3 多处理器并发编程

* 编译器优化的后的汇编指令会丢失原代码的顺序

  * 保证顺序：volatile，barrier

* 中断机制使得 cpu 在不同程序的汇编指令之间跳转，造成代原码的原子性丢失

  * 保证原子性：multual exclusive

* cpu 在执行指令时，为了增加 cache 的命中率，对于没有依赖的指令会进行重排，例如让一个指令等待一段时间后再去执行，从而丢失了可见性

  * 可见性指的是多个处理器访问共享内存的不能做到一致性，例如一个处理器上执行了写内存指令，对于另一个处理器不可见。

  * 保证可见性：fence 指令，原子性指令



# 4 理解并发程序的执行

* 程序 <==> 有限状态机 <==> 有向图

  有限状态即状态的数目是有限的

  一个状态就是一个时刻内存和寄存器的值

  计算机提供的执行即状态转移的条件

* 计算机提供的指令

  * 确定性的指令，deterministic

  * 非确定性的指令，non-deterministic，例如，

    tdtsc/tdtscp，读取 cpu 的时钟计数

    rdrand，读取传感器上的白噪声，真随机

* syscall 也能够带来不确定性

  这是大部分程序的不确定性的来源

* 程序不确定性的来源

  程序接收操作系统的输入，操作系统接收硬件的输入，硬件接受真实物理世界的输入

* 有限状态机，把它当作一个分析处理问题的工具，它有很多的应用

  * 超标量处理器，superscale

    在一个时钟周期内处理一个等价于多个指令的复合指令，相当于在状态之间跳跃

  * 静态分析，例如静态代码检查，代码调试

    gdb 的回退功能

  * 动态分析，检查运行时状态机的执行

* 举例，利用有限状态机可以证明 perterson 算法的正确性

  * 互斥算法的要求两点：

    safety，坏的情况永远不会发生，即不能出现两个同时进入临界区；

    liveness，好的情况永远会发生，即总有一个能够进入临界区

  * perterson 算法

    类比：两个线程是两个人，临界区是厕所，x、y 是两人各自是否需要上厕所的旗子，t 是厕所挂到厕所上的牌子。

    ```c
    // 初始
    x = fasle;
    y = false;
    t = t1;
    
    // thread1
    x = true;                  // pc1=1
    t = t1;                    // pc1=2
    while y && t == t2;        // pc1=3
    do_something();  // 临界区  // pc1=4
    x = false;                 // pc1=5
    
    // thread2
    y = true;                  // pc2=1
    t = t2;                    // pc2=2
    while x && t == t1;        // pc2=3
    do_something();  // 临界区  // pc2=4
    y = false;                 // pc2=5
    
    // 共享的内存 x, y, t
    // 各自的程序计数 pc1, pc2
    // 状态 (pc1, pc2, x, y, t)
    // 坏的情况 (4, 4, _, _, _)，safety 即在有向图中这个状态是不可达的
    // 好的情况 (4, !4, _, _, _), (!4, 4, _, _, _)，liveness 即有向图中这两个状态都是可达的，并且不存在可达的环
    // 画有向图可证
    ```

  * perterson 算法在现代处理器上因为可见性的原因是错误的

    任意一个线程在 load(x), store(x), load(t) 时，因为三者时不同的变量，所以处理器可以进行指令重排

# 5 并发控制

* 从锁的角度来看指令

  * load 指令，读共享内存（睁眼看，不能动手）
  * store 指令，写共享内存（动手写，闭着眼睛）
  * 其它本地计算指令

  只有 load 和 store 指令的影响需要考虑

  假设状态机每次至多执行一次 load 或者一次 store 指令，即指令是原子的

  例如：在 x86 的架构下如果一条汇编执行需要超过一次 load/store，那么在执行时会被拆成更小的指令来执行

  另外：现代多 cpu 架构设计为了使得单线程具有更快的运行速度，使用了多级缓存。

  * 一个 cpu 执行了一个 store 指令，会先写入 cpu 的 store buffer。buffer 满才会写入下一级的缓存，最终到共享内存
  * 一个 cpu 执行了一个 load 指令，同样地它也是先到 store buffer 里面找，其次才去下一级缓存找，最终从共享内存读取。

  为什么这样的设计可行，因为单线程的本地指令执行数量级要远远大于共享内存的相关指令执行数量级，所以更重要的是使得单线程具有更快的运行速度。

* 软件不够，硬件来凑

  由于以上种种原因，纯软件的实现锁复杂性太高，非常容易出 bug。

  相反地，硬件只要能保证哪怕一点点的互斥性，软件上的实现也会变得非常轻松。

* 实现原子性的硬件指令

  test and set, tas

  exchange, xcg

  add one

  load reserve + store sweep, lr+ss

  可以实现一段代码的原子性

* 无论利用怎样的硬件指令实现锁，实现代码中都需要不断地循环



# 6 硬件眼中的操作系统



# 7 并发控制

## 自旋锁存在的问题

### bug 1 锁的公平性

* 无论是单处理器上还是多处理器上，自旋锁可以实现互斥性，但是存在公平性问题：

  两个线程 a，b 分别运行在同一个 cpu 上，其中 a 获得锁、执行任务、释放，a 仍然会大概率获得锁。原因是执行任务的时间区间远大于释放锁后争抢的一瞬间，cpu 在跑完线程 a 释放锁的指令后的一瞬间发生中断概率远远小于在释放锁后，所以 a 大概率会继续获得锁。

  此外，如果是切换出去到其它获取同一个锁的线程，那么这个线程只能够 spin，浪费了 cpu 时间。如果是切换出去到其它处理另外的任务的线程，那么就无所谓。

* 关中断 + spin lock

  默认的前置条件：中断的算法知道一个 cpu 长期没有发生中断，开中断后会马上发起一次中断

  上锁：先关中断，再上锁

  解锁：先解锁，再开中断

  这里的顺序是一个优化点。如果反过来先开上锁，再关中断，那么存在一种可能性——上锁后关中断前，发生中断，cpu 切换到另一个获取锁的线程，这个线程在 spin，会浪费 cpu 时间。

* 开关中断这个操作本身是原子的

  asm volatile 保障开关中断的操作不会和它前后的操作因为编译优化而发生顺序调换

  硬件保证了，中断必定发生在指令边界（类似边缘触发）

  中断前输入全部写入内存/缓存

### bug 2 中断标识位

开关中断的操作可以理解成把 eflags push 到堆栈，或者从堆栈弹出

问题一

* 如果一个线程上锁后，在临界区内又尝试去使用另外一把锁，

  那么在内部的锁释放时就会开中断，这违背了我们的要求

* 使用堆栈的思想，记录第一次关中断时的中断标识位，和关中断的次数（堆栈的高度）

问题二

* 如果在中断处理函数中，某个函数使用锁，那么锁释放后会打开中断。但是中断处理函数要求中断时关闭的。
* 应该在最后一次释放锁，打开中断时，回复初始的中断状态，而不是单纯的打开中断

问题三

* 记录中断结构体应该做成 lock local、thread local、还是 cpu local？

  我认为都可以。lock local 的话，记录每次关中断前的中断标识位就可以，开中断就恢复这个标识位；thread local 或者 cpu local 就使用堆栈的方式好了

  cpu local 是更好的选择，因为线程的数目可能会远高于 cpu 的数目

### bug 3 不可重入

如果一个线程获取锁后，在临界区内又尝试获取同一把锁，这时候它会以为锁已经被别的线程获取了，导致一直 spin



### bug 4 不适用于长临界区

其它尝试获取锁的线程或者处理器会进入 spin。严重的情况下，一核持有，七核围观

硬件的中断不能够及时响应



## xv 6 的解决方式

* 简单：

  尝试获取锁时，如果发现锁已经被占用，那么让出 cpu（软件中断）

  如果等待锁的线程非常多，那么很有肯能把大部分线程空转一轮，才轮换到一个可执行的线程

* 进阶：

  尝试获取锁失败后，把当前线程标记成 blocked，并且压入队列、让出 cpu

  释放锁前，从队列取出一个线程，把它标记成 rannable

  中断处理程序每次收到一个中断时，从 rannable 的线程中选择一个运行



# 8 并发数据结构 malloc/free 算法

## 编程建议

* 白板手书，减少涂改，锻炼思路
* 不言自明

## list head

嵌入到其它对象当中的双向循环列表

```c
struct list_head {
  struct list_head *prev, *next;
};
```

* 应对一个对象在多个链表里的情况

  ```c
  struct task {
    struct list_head wait_queue; // for mutex
    struct list_head task_list;  // for procfs
  }
  ```

* 内存友好：

  * 链表跟着对象走，内存分配时一次性分配完成，减少内存碎片
  * 局部性友好：访问对象里的链表，大概率也会访问对象中的其他数据（直接通过地址偏移量访问），利用了 cpu 缓存的特性。

## raii

resource acquisition is initialization

一种常见的编码规范

例如定义一个锁，在类的构造函数上上锁，析构函数上解锁。通过这种方式来避免遗漏解锁操作。



## sbrk 和 mmap

操作系统提供的内存分配调用

* brk 即 break，表示对内存空间设限，把内存类比数组，brk 就是允许使用的最大 index



## malloc 和 free

* 设计 malloc 和 free 的本质是维护一个被分配的区间集合

* premature optimization is the root of all evil

  不要脱离 workload 来做优化

* 多处理器上 malloc 和 free 的 workload

  * 非常频繁的小对象（几到几十 bytes）分配和释放
  * 比较频繁的中等对象（几百 bytes 到几 KiB）分配和释放
  * 偶尔的大内存分配（MiB 级别）



## fast path & slow path

一种设计原则

fast path 快糙猛，但是小概率失败

slow path 缓慢精细，保证成功



* 大内存分配考虑在线程本地分配，使用链表、interval/radix tree或者 bitmap
* 小内存到中等大小内存考虑使用 slab 分配器



## slab 分配器

* 对常见的分配大小分设置规格，每个规格固定大小

  节约了每段内存的头信息

* 设置 per -thread 分配缓存

  只考虑 malloc 和 free 之间的并发

  可以 lock-free 实现，也可以锁 slab 实现



# 9 并发控制

## 同步

在某个时间，就某个或某些状态达成一致



## 条件变量

* `wait`

  如果条件不成立，那么等待；

  进入等待时，wait 会释放锁；

  竞争到 notify 信号时，获得锁，结束等待

* `notify`

* `notifyall`

信号如果没有被捕获，那么会丢失

```c
// 保证条件变量正确使用的代码模板
mutex.lock();
while (!condition) { // 使用 while 的意义在于被唤醒时，condition 不一定成立
  wait(&contion_variable, mutex)
};
do_your_job();
notifyall(&contion_variable);
mutex.unlock();
```



## 信号量

* 某些简单的场景下，可以使得代码编写得更为简洁，但是大部分情况下容易使用出错。

* 信号量 == 锁 + 条件变量

  信号量不会丢失



# 10 并发 bugs

## 调试的基本原则

* 机器是对的，99.99 %
* 未测代码是错的，90 %



## 调试理论中的 fault, error, failure

* fault，指 bug，难以被观测到
* error，指运行时的状态错误，难以被观测到
* failure，指可观测的执行结果错误



## debug 时的两条关键路线

* fault ==> error 需要更多的测试尽可能地把 fault 转化成 error
  * 构造面面俱到的测试用例
  * 构造压力山大的 workload
* error ==> failure 需要更多的检查尽可能地把 error 转化成 failure
  * 日志信息
  * 防御性的 assertion



## 难以定位的并发 bugs

* 分类标记：

  ABBA 死锁

  ABA 原子性违反 atom violation

  BA 顺序性违反 order violation

* 一部分是死锁导致的 bug

  比重不高，原因是死锁这样的 fault 相对容易转化成 failure

* 大部分是原子性违反/顺序性违反导致的 bug

  * 原子性违反，忘记上锁

    没有对存在竞争的共享数据加锁

    存在恶意的竞争

  * 顺序性违反，忘记同步

    程序的执行顺序不符合预期，例如 instant message 项目中流的终止代码



## fault --> error

* 一个手段是构造复杂的 workload

* 一个非常有效的手段是插入 delay

  让并发 bug 更加容易暴露出来



## error --> failure

* 检查

  assertion

  计数器检查

  标记/上色检查

  金丝雀检查，例如在栈前后增加空余内存并上色，以此来保护栈溢出

* 动态程序分析

  简单理解就是运行时打印日志并且分析日志

  * 在程序运行时进行观测和检查

  * 在适当的时候对程序的行为进行调控

  例如 lockdep 预测是否存在 ABBA 的 bug

  lockdep 的原理就是记录所有的锁的调用顺序，找到可能发生问题的顺序，给出预警

* 常用工具

  gcc santitizers

  * adressSantitizer
  * memorySantitizer
  * threadSantitizer
  * ubSantitizer, undefined behavior



# 11 进程的抽象

## 操作系统

操作系统模拟了所有进程的状态机

* 所有物理内存对操作系统可见
* 所有寄存器对操作系统可见



操作系统是一个中断处理程序

* 被动的中断，时钟、IO 设备、NMI 等
* 主动的中断，应用程序调用 syscall



## 进程

进程的运行的任意时刻都是进程状态机的一个状态

* 仿佛是独占的

  虚拟化（内存、cpu 等）

  不可访问系统寄存器

  ...

* 进程可以持有操作系统的对象



三类系统调用

* 进程管理
* 内存分配
* 文件读写



##进程管理

### `fork`

* fork bomb

  ```sh
  # shell 允许 : 作为标识符
  # 提示，定义了一个函数，并且执行了它
  :(){:|:&};:
  ```

  

* 从状态机的视角理解，fork 是对状态机做一次完完全全的拷贝，处理父子进程中返回的进程号不同

  > example 1

  ```c
  pid_t pid1 = fork();
  pid_t pid2 = fork();
  pid_t pid3 = fork();
  printf("%d %d %d\n", pid1, pid2, pid3);
  
  // 输出是怎样的？
  // 提示，画状态机
  ```

  > example 2

  ```c
  # define n 2
  
  int main() {
    for (i = 0; i < n; i++) {
      fork();
      printf("hello world\n")
    }
  }
  
  // 输出是怎样的？
  // 提示，画状态机
  ```

  



# 16 处理器调度：时间片、RR、MLFQ、CFS

rr 这里指的是 round robin



建模：寻找 cost，好的调度算法就是让 cost 最低



## 中断出现前（无法分时复用）

* 最短等待时间 shortest job first

  任务的等待时间是任务执行完成的时间减去任务到达的时间的差

  那么我们希望平均的等待时间最短。

  

  平均等待时间作为 cost

  

  例如：假设只有一个 cpu，现在我们有三个任务 a，b，c，它们的需要的执行时间分别是 1s，5s，10s，他们同时到达。

  这种情况下，按照 a，b，c 的顺序执行平均等待时间最短。

  

  把任务的发起者类比位顾客，等待时间越长，满意读越低。那么追求最短等待时间实际上就是让顾客的平均满意度最高。

  

* 以上所说的情况是简单的任务同时到达，实际任务到达的时间是不确定的

  允许任务的抢占，即暂停还需要很久才能完成的长任务，执行短任务，后续再长任务

  shortest time to completion first



## 中断出现后（能够分时复用）

### 基础假设

* 线程做的事情

  计算相关，io 相关的

* 随时有线程创建/销毁
* 能够以固定的频率发生中断处理器



### 调度方式

* 最简单的调度方式，round-robin

  round-robin 的问题在于操作系统中部分线程要求快速地响应，

  如果同时还存在大量其它的线程，round-robin 依次序地把时间片分给所有线程，那么要求快速响应的这部分线程就没有办法快速响应

* 引入优先级，linux 操作系统用 nice 表示优先级

  nice，类比人，越好说话越容易被插队

  nice 的值越高优先级越低

  nice 的值每相差 1，分到的 cpu 时间比率相差约 1.25 倍，前提是两个线程在同一 cpu 上运行

  nice 的实现方式即时间轮

* 如果时人为地指定优先级，那么是比较费力的

  如何做到自适应地调整？

  要求快速响应的线程通常会使用到 io 相关的系统调用，那么我们可以在运行时统计包括 io 相关的一些类似的具有指向性的系统调用或者操作系统指令，然后自适应地调整 nice 的值

  

  一种实现，多级反馈调度 mlfq

  * 设置多个调度队列
  * 每个队列的优先级不同
  * 优先调度高优先级队列
  * 如果统计到一个线程运行时有较多的 io 操作，那么它更可能是交互性的线程，调高他的优先级；相反地，调低优先级

* cfs，complete fair scheduling

  名字有点厉害，哈哈，让我们来看看它是否名符其实

  核心思想：

  * 尽可能的让每个线程<u>已经</u>使用的时间片长度接近，即每次中断后选择已经使用时间片长度最小的线程运行，补偿它，让它后续有机会追上来
  * 同时因为考虑到不同的优先级，所以抽象了一个 vruntime，即虚拟运行时间的概念
  * 补偿的时间也和优先级成比例

  复杂的情况：

  * fork，父子进程唯一的不同是返回的 pid，那么子进程继承父进程的 vruntime

    tricky 的地方在于，父进程拿到 pid 比子进程快（父进程只需要子进程的 pid，子进程还需要等待 fork 完成复制才会被创建），所以子进程 fork 完成后，父进程实际会又多执行了一点点，那么子进程在下次中断时就能够优先于父进程被调度以便完成后续的 excve 系统调用。

  * 如果有一个线程 sleep 了太久，那么唤醒它的时候不能够给它太多的时间片（虽然这样它有点吃亏），不然其它线程会饥饿。

    但是从另一个角度想，既然睡了那么久，那么接下来大概率也没有计算密集的任务需要太多的 cpu 时间

    linux 的实际操作方式是选择一个除了它自己以外最小的 vruntime 赋值给它

  * 考虑锁/信号量的影响与应对

    todo

  * 考虑有极短（比单个时间片短很多）的 io 等待

    todo

  数据结构：

  * 用什么样的数据结构去实现 vruntime

    这个问题考虑方式就是对该数据会做哪些操作，频率如何？

    * 经常找最小值
    * 经常增删

    ==》考虑堆

    实际 linux 中使用了复用了已经实现的红黑树

  * vruntime 溢出如何处理 ，类型为 unsigned int64，单位 ns

    利用了一个合理的假设：

    任何两个 vruntime 相差不会超过 unsigned int64 的一半，那么直接做差能够 wrap around

    始终使用 time delta 的正负来判断，具体细节待研究

  

  

# 17 处理器调度：优先级反转、多处理器调度

## 优先级反转

实际运行时发生的一种不符合预期的行为，低优先级的线程比高优先级的线程获得跟多的 vruntime



产生的原因：

假设有三个线程 a、b、c，nice 值分别为 -10、0、10，a 和 c 会竞争同一把锁，那么就可能出现这样的情况：

如果 c 获得了锁，那么 a 无法获得锁于是主动让出 cpu，又因为 b 的优先级高于 c，所以导致出现 a 的 vruntime 小于 b 的 vruntime 的现象，并且这种情况难以得到缓解。

虽然 a 等待 c 时合理的，但是造成 a 的 vruntime 小于 b 的 vruntime 是不符合预期的。



处理手段：

方式一，提升 c 的优先级。为什么不降低 a 的优先级，因为 a 的优先级高，代表 a 时重要的任务，重要的任务不应被降低优先级。

方式二，维护线程间的依赖关系。这种方式相对复杂。

方式三，高优先级的锁可以剥夺低优先级的已经获取的锁。



## 多处理器调度

非常复杂的问题，面临诸多的挑战



### 多用户多任务

考虑这样的情况：用户 a 只跑一个任务，这个任务起 1 个线程；用户 b 跑一个任务，这个任务起 1000 个线程。如何做到公平？

实际中的解决方案，cgroups，即 control groups



###多核处理器的每个核的处理能力不均等

现代处理器存在大核、小核之分，有些还存在专用核

即异构的处理器架构



### 非均匀的存储访问

共享内存的底层是 memory hierarchy，cpu 访问不同 level 的缓存显然性能是不同的

那么 producer 和 consumer 线程被调度到使用同一块 memory 和不同块 memory 的性能差距可能很大



### cpu hot plug

动态地添加或者移除 cpu



### 应对

* 更加精细的建模

* 试图预测

  branch prediction

  prefetching

  speculative/run-ahead execution

* 决策

  采用怎样的调度策略



# 18 demand paging: 动态链接的礼物

## mmap 的实现

* 虚拟内存，进程的代码在执行的时候，进程看到的地址空间。进程是通过物理地址上的映射函数 f 来看到虚拟的地址空间的。

  除了少数的存放在物理地址上的内容，例如 f，进程是不能直接访问物理地址的，而是通过 f 去访问物理内存

  * 显然我们对这个 f 的性能要求很高，这个映射函数 f 是通常是由 radix tree 或者 hashtable 之类的数据结构维护的

  * f 的作用就是将虚拟内存映射到物理内存

  * f 对应的数据结构当然是维护在物理内存上的，在物理内存的某个页表上

  * 进程访问 f 当前维护的范围之外的地址 a，就会产生 page fault

* 操作系统（内核），既可以看到虚拟地址空间，也可以看到物理地址空间

* 分页机制

  操作系统改变 cr3 寄存器的值，来切换不同进程映射内存的函数 f，进程从 cr3 中存储的地址找到自己的 f

  > 那么到底进程是否能够访问地址 a？

  操作系统在 page fault 发生后，地址 a 会被放到 cr2 寄存器。操作系统会去进程的数据结构里查看 range 记录，有没有对地址 a 的访问权限，即检测 cr2 中存放的地址 a 是否合法

  range 记录中是虚拟地址的范围，以及这段地址中应当存放的内容的位置，比如

  * 可执行二进制文件的虚拟地址 --> 它在物理存储上（磁盘/硬盘）的地址，访问权限
  * 进程的堆栈分配的虚拟地址 --> nil，访问权限

  > 如果是地址 a 合法的，那么操作系统就会

  * 分配一个物理页面
  * 把物理页面和虚拟页面的映射添加到 f 中
  * 给物理页面填上应该填入的内容

  > 如果是地址 a 是非法的，那么操作系统就会
  
    产生 sigsegv 信号，即 signal segment valiation，进程收到该信号的默认行为是终止退出

操作系统操作的进程的数据结构当然也是维护在物理内存上的

* 每个进程的地址空间中都存在一段区域是映射到操作系统内核代码的

  进程没有权限访问这段区域

  备注：所谓内核态就是拥有特权，保存特权指令、特权寄存器、特权...

## demand paging，反向使用

swapping，将某个/些进程暂时不用的内存页交换到物理存储上，空出来给其他进程使用

这里有一个细节，交换到物理存储上的页的指针存储在哪里？

分页机制支持存储少量的额外信息，todo

swapping 机制的出现是因为早期计算机内存紧张、昂贵。

现在，内存增长，以及 swapping 本身的性能消耗，导致 swapping 的需求减小。一般只在高内存负载的情况下，才考虑使用。

swapping 的策略？

如何保证调度的公平性？

> 策略一：预测内存的使用情况

比较好

把未来大概率不使用的内存页 swap out

把未来大概率使用的内存页提前 swap in，即 prefetch

page fault 时 swap in

> 策略二：fifo

不能应对 hot 的内存页

> 策略三：random

比较中庸的策略

> 策略四：lru

不太好实现，需要借助内存硬件提供的标志位

D，dirty，是否已经被写入过

A，access，是否已经被访问过

## 共享库、动态链接加载

* got，global offset table

  got 存放动态链接的函数的地址

  got 中的如果存在同名的函数，那么采用的是先到先得的方式

* LD_PRELOAD 环境变量，指定优先加载某些动态链接库

* todo

  ```sh
  man 8 ld.so
  man 3 dlsym
  ```

