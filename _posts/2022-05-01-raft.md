---
layout: post
title:  raft
categories: raft
---

# raft 的定义
管理日志副本的共识算法

# raft 的目的
* （与 paxos 相比）便于工程实现
* （与 paxos 相比）便于教学，即易于理解

> 为了达到上述两个目的

* raft 被划分成多个部分
  * leader election
  * log replication
  * security
  * membership changes

* raft 优化了状态空间
  * 减少了状态数目
  * 减少了达到不一致的路径数目

# raft 的特征

* strong leader
  * log 只会从 master 流向 follower

* fast election
  * 通过随机的定时器（具体的，是随机的 election timeout），简单快速地解决达成共识前的冲突（具体地，是 vote split）
  * 在网络通畅的情况下，一个 candidate 能够迅速的拿到所有 request vote 的响应，如果这个 candidate 没有拿到多数票并且发生了 vote split，那么这个 candiate 不会立即发起新一轮的投票，而是等到 election timeout 发生才开始。否则立即开始会产生严重的 election race

* membership changes
  * joint consensus approach，新老集群共同决议

# 复制状态机
replicated state machine

## 架构

> client

* client 的请求会被转发到 leader 上

> leader

* leader 全权负责 replicated log 的管理
  * 接受从 client 发来的 log entries
  * 复制 log entries 到其它的服务器
  * 判断何时能够安全地把 log 应用到状态机上
* leader 的 log 是 append-only 的，绝不会删除，也不会重写

> follower

> client --> consensus module --> replicated log --> state machine --> client

* consensus module 的作用就是保证不同服务器上的 replicated log 完全一致（log 上记录的命令一致、命令的顺序一致）

> 为了达到上述目的，共识算法要求

* 在 non-byzantine 情况下，无论遇到什么失败（网络延时、网络分区、重复、重排序、丢包），也绝不返回错误的结果
* 如果超过半数的 server 是正常的，那么就能够正常对外提供服务。此外，异常的 server 也可以基于持久化数据来恢复，重新加入集群
* 不对时间有强依赖，这样可以避免时钟错误、消息延迟等极端情况带来的影响
* 如果超过半数的 server 已经达成共识，那么这次共识的结果就可以对外了，其余的较慢的 server 不会拖累整体性能

# paxos 的问题

# 理解驱动设计
design for understandability

> 易于理解的一个重要好处是，便于在工程中对算法实现进行拓展

> 达到易于理解这个目的的两个方式

* 分解
  * leader election
  * log replication
  * security
  * membership changes
* 简化
  * 减少状态数目，让系统更加紧凑（coherent）
    * 日志不允许有 hole？日志的 entry id 是连续递增的
    * 减少日志可能达到不一致的路径数目
  * 简化选举过程
    * 引入随机性（chose any, it dosen't matter）

# raft 算法细节

## leader

> leader 全权负责 replicated log 的管理

* 接受从 client 发来的 log entries
* 复制 log entries 到其它的服务器（并行的 rpc 调用）
* 判断何时能够安全地把 log 应用到状态机上

> 在 leader 的基础上，需要解决三个字问题

* leader election
  * 一个 leader 失败时，选举新的 leader
* log replication
  * leader 将自己收到的日志扩散到集群中
  * leader 强制要求其它 server 的 log 与自己的保持一致
* safety
  * 如果任何一个 server 已经将一条 log entry 应用到它自己的 state machine，那么其它的 server 也必须将同样 log index 的、同样 log entry 应用到它们各自的 state machine

## term

> term，可以理解为任期

> 任期的定位是逻辑时钟

* 任期的长度是任意的
* 任期的以一次成功的选举开启
* 任期用连续的 integer 标志
* 任期内至多有一个 leader
  * 如果出现 split vote 的局面，这个 term 就没有 leader，很快之后重新开始一轮选举

> side effect

* 不同的 server 可能在不同的时间观察到任期间的过渡期

## server state

> 对于任意的 server，都需要被持久化的 state

* term
* voted for
* log[]

每次 leader 回复 client 之前，或者 server 回复 rpc 之前都需要进行持久化操作

> 状态转移图

* follower
  * in
    * server 启动 --> follower
    * candidate 发现 current leader 或者 new term --> follower
    * leader 发现 higher term --> follower
* candidate
  * in
    * follower timeout，开始选举 --> candidate
    * candidate timeout，重新开始选举 --> candidate
* leader
  * in
    * candidate 收到过半的 votes --> candidate

## rpc

> raft 算法中最少只需要两种 rpc

* requestVote，cadidate 发起投票
* appendEntries，leader 发起 log entry replicateon，也用作心跳

> 其它 rpc

* transferSnapshot

## leader election

> 第一轮

* 各个 follower 随机地在固定的范围内（150-300ms）选取 election timeout
* 在这个期间内没有收到有效的心跳，就超时，然后自增 term 并且发起新的选举，进入 candidate 状态
  * 有效即要求：心跳中的 term 大于等于当前 server 的 term
  * candidate 发起选举投票总是投自己

> 第 n 轮

* 如果发生 vote split，那么各自重新发起选举，随机地选在固定范围内（150-300ms）选取 election timeout
* 一定要等到 election timeout 然后再发起选举。否则立即重新发起选举会造成强烈的选举竞争，导致循环的 vote split，长时间选不出 leader

> 投票的策略

* 收到投票请求的 server，会比较投票请求中 term 与自身当前 term 的大小。对于有效的投票请求，采取谁先来投谁的策略
* 如果一个 candidate 收到过半的选票，那么它立即会向其它 server 发出 heartbeat，尝试建立自己的权威，即阻止发起新的选举
  * 进入 leader 状态时，必须立即发出 heartbeat，立即！立即！
  * 只有心跳中携带的 term 是集群中最大的，权威才能够建立起来
  * 为了避免被移出集群的 server 发起的选举干扰，follower 在 election timeout 前，会忽略任何的 request vote
  * 心跳的最大间隔 <= 最小 election timeout 的一半。这样如果 leader 正常， follower 在 election timeout 前会至少收到 1 次 heartbeat

> candidate 在 election 的过程中如果收到其它服务器发来的心跳，即 appendEntries，那么就会比较自己当前 term 与心跳中携带的 term 的大小，判断其权威新，如果对方的 term 大于等于自己的 term，那么自己就放弃 election，转变成 follower

## log replication

> log 中的每一条 entry 有三个字段组成

* log index
* term
* state machine command

> log replication 流程

* client 发起请求
* leader 收到请求，append 到自己的 log，然后向所有其它 server 并行地发起 appendEntries 调用
* 当 entry 被安全地复制后（超过半数的 follower 已经回复成功），leader 将这条 entry 应用到自己的状态机，然后向 client 回复
* 即使 leader 已经回复 client，但是如果有任何 follower 没有及时的回复（follower crash、follower 跑的慢、网络问题等），leader 会不断地向这些 follower 发起重试，直到成功

> log matching property

* 如果两个不同 server 上的 log entry 拥有一样的 log index 和一样的 term，那么这两个 log entry 的 command 一定是相同的
* 如果两个不同 server 上的 log entry 拥有一样的 log index 和一样的 term，那么这两个 log 中两个 log entry 之前的 entries 一定是相同的
 
> log inconsistency

相比于 leader， follower 可能

* 缺失 entries
* 多出 entries
* 包含不一致的 entries

> raft 处理不一致的方式是强制不一致的 follower 复制 leader 的 log，这也就意味着 follower 中的 log 会被重写

> 如何确定不一致的开始位置？follower 会根据 appendEntries 中的 prevLogIndex 和 prevLogTerm 做一致性检查，不一致则拒绝。然后

* 方式一：单纯的依赖 leader 在收到拒绝后，把 index 减一，并充重试
  * 这种方式有多少冲突的 entries 就需要重试 appendEntries 多少次
* 方式二：follower 回复的时候也携带上一些信息（follower 的 log 上发生冲突所在的 term，以及该 term 期间的第一个 log index），供 leader 来判断 index 减小多少
* 其它方式：二分搜索
  * 这种方式有多少冲突的 terms 就需要重试 appendEntries 多少次
* 方式一通常是足够的，failure 是偶发的，不一致通常不会太多

> 需要注意的是，这里 follower 虽然拒绝了 appendEntries，但是这仍然是一次有效的 heartbeat，必须要重置 election timeout。否则当 follower 落后太多事，频繁的拒绝，会造成该 follower 发生 election timeout 进入 candidate 状态发起选举，干扰到原本正常的 leader 复制 log 行为

## 安全性

> follower 的 log 可能会被重写，而 follower 先前也可能是 leader，而 leader 有 commit log entry 的权限，即修改状态机的权限，那么如何保证状态机一致？

* leader commit completeness property 是确保 state machine 一致的另一个重要约束

 所谓 leader commit completeness property 即 leader 拥有全部的一提交 entries

> 如何达到 leader commit completeness？

* voting 的过程中，一个 candidate 除了得到超过半数的 votes，还必须拥有全部的 committed log entries，它才能够成为 leader

* requestVote 中携带 candidate 的 commit 信息，voter 会比较自己和对方的 commit 信息，只有满足 candidate 的 commit 信息比自己的更 up-to-date，voter 才会给这个 candidate 投票

* 这样只要 candidate 获得了超过半数的选票，就说明具备 commit completeness
  * 最一开始没有 log entry，满足 commit completeness
  * 后续 raft 始终确保超过半数这一点，那么给 candidate 投票的超过半数的 server 其中之一必然具备 commit completeness。这个具备 commit completeness 的 server 既然给 candidate 投票了，表示它确认了这个 candidate 也是 commit completeness 的

> 如何判断谁的 log 更加的 up-to-date？

* 比较 term，term 越大，越 up-to-date
* term 相同，比较 log 长度（log 即末尾的 entry index），长度越长，越 up-to-date
  
  server 在收到 request vote 时，会检查这两点，只有 request vote 的发起者具有更 up-to-date 的 log，才会把票投给它

> 为什么不通过比较 committed entry id 来确定谁更 up-to-date？

* follower 的 committed entry id 是滞后于 leader 的（至少需要 2 次 rpc 才能同步），只检查 majority 无法得到 最大的 committed entry id，必须要检查全部。检查全部时完全不可行的

> 提交 log entry 的条件？

* 条件一，entry 在 server 中的副本数超过了半数
* 条件二，entry 的 term 等于当前 leader 的 term

  此时，根据 log matching property，该 entry 之前的所有 entries 也都被自动提交了，无论它们的 term 是当前的 term 还是之前的 term

> raft 为什么要求提交条件二？

* 最重要的原因是：仅仅要求条件一，可能出现已被提交的 entry 被新的 leader 强制重写
* 可以保证在提交属于 previous term 的 log entries 时，不需要改变它们的 term number
* 便于追踪每个已经 committed 的 entries 是在哪个 term 添加的
* 新的 leader 操作属于 previous term 的 entries 时，处理的数据量更少

> 当然也存在其它提交属于先前 term 的策略，比如如果一个属于先前 term 的 entry 在每一个 server 上都存在副本，那么认为它是已提交的。虽然 raft 提交属于先前 term 的 entries 的策略相对保守，但是简单高效。

## follower 和 candidate crash

应对 follower 和 candidate crash 的策略相对简单

* 发现失败，无止尽的重试，直到成功
* appendEntries 和 requestVote 均具有幂等性

## 时间和可用性

* 一方面 raft 不能够依赖时间
  * 不因为事件发生的快慢而作出错误的判断
* 一方面可用性要求对 client 的相应需要及时
  * 如果 leader 不存在，那么无法响应 client，那么响应时间就主要取决于选举的时间就
  * 如果 leader 存在，那么响应时间取决于 entry 扩散到超过半数的 server 的时间。而 leader 既然存在，就表明集群的状态相对健康（leader 周期性地发送心跳），entry 的扩散时间一定相对较短

> 通过上述分析，election 的时间对于 availability 至关重要

> election 能够正常运作的时间约束：`broadcastTime << electionTimeout << MTBF`

* `broadcastTime`，leader 并行向所有 server 发起 rpc 调用并且收到它们的响应的平均时间
* `MTBF`，单个服务器发生失败的平均间隔（这一条主要针对 leader）

> `broadcastTime` 和 `MTBF` 都取决于底层系统，`electionTimeout` 由使用者选定

* `broadcastTime` 主要由网络延迟 + 持久化时间决定，`broadcastTime` 一般在 0.5-20ms
* `MTBF` 一般在几周、几个月的量级

> 如果时间约束成立，那么近似地 `avalability = electionTimeout/MTBF`

## 集群成员变化

cluster membership changes

> 如果不提供集群成员变化，那么只能够通过 cluster 的改配、重启来处理

> 将集群成员变化加入 raft 算法的初衷

* 提高 avalability
* 减少人为干预，降低出错概率

> 如何在 raft 中实现集群成员变化？

* 把配置变化当成状态机命令
* joint consensus，联合决议
  * 即当前 leader 同时执行新老配置，并且要求任何决议只有新老双方各自都通过，才算通过
* 一个 server 只要收到了更加 up-to-date 的配置，它就使用这个更加 up-to-data 的配置
* 在 `C_old` 和 `C_new` 之外引入一个过渡状态 `C_old_new`

> 存在的问题

* 新加入的 server 需要复制历史日志，这会耗费一定的时间
  * 解决方式：引入一个 non-voter 身份给新加入的 server
    * leader 只向 no-voter 发起 appendEntries 调用，不发起 requestVote 调用
    * 当所有的 non-voter 追上了最新日志，reconfiguration 才开始

* 在一个特定时期，可能存在一个 leader，这个 leader 不属于新配置中的成员，而是只属于老配置中的成员，即一个即将被移除的成员
  * 解决方式是：当这个 leader 提交了 `C_new` 后，立即自动放弃 leader 身份


* 即将被移除的 server 会频繁地 election 超时，发起 election，从而打断正常的集群行为，影响可用性
  * 解决方式：从 follower 收到当前 leader 的心跳开始计，在 election timout 前，拒绝其它 candidate 的 request vote 请求