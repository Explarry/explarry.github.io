---
layout: post
title:  gfs
categories: gfs
---

# gfs

google file system

## 设计目标

* 要求组件不可用是正常情况，而不是异常情况
* 要求适用于存储大文件，数 G 的大文件
* 要求写操作以原子的追加数据为主，而不是修改现存的数据
* 要求应用层和存储层协同

## 基本假设

* 组件通常是便宜的商用产品
* 存储的通常是大文件，支持存储小文件，但是不会追求小文件的效率
* 应用读操作大部分是顺序读，支持随机读，但是不会追求随机读的效率
* 应用写操作大部分是追加，支持随机写，但是不会追求随机写的效率
* 支持并发的原子性追加，减小原子性带来的 overhead 是优化的重点
* 应用对吞吐量的要求很高，但对响应的及时性有一定的容忍度

## 接口

* create
* delete
* open
* close
* read
* write

* snapshot
* record append

## 架构

* 1 master + n chunkserver
* n client

> master 的目的：能够基于全局信息来解决存储在哪和副本怎样管理这两个的复杂问题

> master 的 metadata：命名空间，访问控制，文件到 chunk 的映射，chunk 的位置，lease 的管理，垃圾回收，chunk 的迁移

> 文件在存储时被划分成大小固定的 chunk，默认副本数为 3，不同的 namespace 可以设置不同的副本数

> chunk server 和 client 都不会缓存文件数据。chunk server 利用操作系统 buffer，client 缓存对于大文件收效甚微

* 控制面
* 数据面

> master 只负责全局的控制，master 不参与任何文件数据的传输
> client 和 chunk server 之间进行文件数据的读写

## master metadata

### master 内存中的数据

* 放到内存中，方便定期扫描、变动
* 每 64M 的 chunk 对应小于 64 bytes 的 metadata
* 每文件命名空间/文件名对于小于 64 bytes 的 metadata，这里使用了前缀压缩算法

### chunk location

* master 不会持久化 chunk location 数据，核心的逻辑在于 chunk server 对持久化后的 chunk 有最终话语权
* master 每次启动时向 chunk sever 查询各个 chunk location

### 操作记录

* 操作记录只有既记录了本地日志，也记录了几份远程日志，才会给 client 返回
* 为了吞吐量提高，使用了 batch 处理，即积攒几条记录一起 flush
* 使用 checkpoint 避免操作日志过大
* checkpoint 采用 compact b-tree like 数据结构，可以直接映射到内存使用
* 创建 checkpoint 需要花费少量时间，这要求 master 的内部状态采用的数据结构不会推迟新来的变更

## 一致性模型

### gfs guarantee

* 实现 gfs guarantee 凭借的是
  * namespace lock
  * global operation log
  * chunk version number，可以用于判断 chunk 是否 stale
  * stale chunk 永远不会在 master 询问 location 时被返回、永远不会参与后续的 mutation，相反地它会被垃圾回收

* 决定一致性状态的三个要素
  * 操作类型
  * 串行/并行
  * 成功/失败

||write|append|
|:-|:-|:-|
|串行成功|defined|`defined, interspersed with inconsistent`|
|并行成功|consistent, undefined|`defined, interspersed with inconsistent`|
|串行失败|inconsistent|inconsistent|
|并行失败|inconsistent|inconsistent|

* write: 在应用指定的 offset，写入数据

* append
  * offset 由 gfs 选择，在并发的情况下，保证原子性，保证至少写入一次
  * gfs 会返回一个 offset，这个 offset 在所有的 replica 上都是一样的，它代表一段 defined region 的开始，并且这个 definde region 必然包含被 append 的 record
  * 这也就意味着，gfs 可能会插入少量的 padding 或者 duplication
  * 返回 offset 保证了以 offset 开始的一个 region `defined`，但是少量的 padding 或者 duplication 又导致了 `interspersed with inconstent`
  * replica 不是 byte-wise identical，而是 at least once atomic append
  * padding 是为了保持固定的 chunk size，64MB

* 状态定义
  * consistent 状态：对于同一个 file region，无论从哪一个 replication 读取，所有 client 都能够看到同样的数据
  * defined 状态：前置要求 consistent，其次 clients will see what the mutation writes in its entirety（这里如何理解？）
    * 所有的 mutation 都被应用了，但是可能存在少量的 padding 或者 duplication

* 存在的问题
  * client 会缓存 chunk location，但是这个问题的影响不大，原因是：
    * client 缓存过期时间短
    * 大部分是 append 操作，即使读到 stale chunk，也只是会提前结束，缺失部分数据
    * 机审不是 append，也可以纠正，通过后续向 master 重新询问 chunk location
  * mutation 成功，但是后续 chunk server 可能发生数据损坏，但是这个问题影响不大，原因是：
    * 通过 checksum 识别损坏的数据，损坏的数据不会被返回，还可以从其它的 replication 读取
    * 会尽快从其它的有效 replication 恢复一个新的 chunk
    * 即使短时间内所有的 replication 都损坏了，应用也只会收到数据不可用的错误提示，而不是收到损坏的数据


### 对应用的隐含要求

* 尽量使用
  * append
  * checkpoint
  * writing self-validating
  * self-identifiying record

* reader 处理正确处理额外的 padding 和 duplication
  * 可以采用 checksum 达到这一目的
  * 使用唯一标志来过滤掉 duplication

## 系统交互

### lease

lease 即条约，设计条约的目的是减少 master 的负载

> 条约的设计要点

  * master 生成条约
  * master 下发条约，下发到的第一个 replication 为 primary
  * primary 决定 mutation 的应用到 chunk 的顺序
  * 条约存在过期时间，初始过期时间是 60s，primary 可以向 master 申请续期
  * 条约的续期申请和相应被负载在（piggybacked on） master 和 chunk server 之间的 heartbeat 上进行传输
  * master 可以撤销 lease，如果失去和 primary 的连接，那么可以等条约过期后重新下发一个条约给其它 replicaton


> 典型流程

* client 向 master 询问 lease 和 location 所在。如果没有 lease，那么 master 生成并下发
* client 缓存 lease 和 location 所在。如果 primary 不可达或者 primary 回复不再持有 lease，那么重新向 master 询问
* client 向 replica 传输数据，传输的顺序是任意的
  * 可以解耦 data flow 和 control flow
  * 可以基于网络拓扑进行传输路径优化
* replica 收到数据就 lru，知道数据被使用或者过期
* 如果所有的 replication 向 client 恢复收到数据，那么 client 就向 pirmary 发起持久化请求，primary 决定 mutaton 的顺序，和开始的 offset
* primary 向二级 replica 转发持久化请求，二级 replica 按照 primary 指定的顺序和 offset 开始持久化动作
* 如果所有的 replica 完成了持久化，那么操作就完成了
* 如果有任何的错误发生，那么 primary 向 client 报告，client 发起重试


### data flow


### atomic append

* 如果 primary 把 record 追加到当前的 chunk 时发现 chunk 的大小大于 chunk size，那么会 primary 会对当前 chunk 做 padding，并且下发给所有二级 replica。在这之后，才会在下一个新的 chunk 上发起重试 append 操作
* 一个 record 最大不可超出 chunk size 的四分之一，这是为了保证不会有太高的碎片率（太多的 padding）

### snapshot

> 要求

* 在短时间内创建一个文件或者目录的副本
* 不会打断正在进行的 mutation

> 采用 copy on write 来的达到上述要求

> 典型流程

* master 取消在 snapshot 范围内的 lease，或者等待 lease 超时
* master 将 snapshot 范围内的 metadata 在内存中拷贝一份，此时这两份 metada 会指向相同的 chunk location
* 当有新的写操作时，client 先向 master 查询 lease 的持有者。此时，master 会检查待写入的 chunk handler 的被引用数。如果被引用数大于 1，那么就会先创建一个新的 chunk handler 副本，并且要求所有持有这个 chunk 的 replica 创建一个 chunk 副本。在此之后，才会回复 client，继续正常的写入流程


## master operation

master 的 operation 包括
* 管理 namespace
* 决定副本位置
* 创建、重建副本、重新负载均衡
* 垃圾回收
* 检测 stale 副本

### 管理命名空间

> 设计要点

* 通过不在目录中存储目录中有哪些文件或者目录，来减小数据竞争
* 不支持别名，不支持类似于 unix 中的软/硬链接
* 用 lookup table 的方式组织数据，每一项代表一个 full pathnames to metadata 的 mapping
* 通过 prefix 压缩来节约内存空间占用
* 每一个节点都有读写锁，通过减小锁的粒度，来提升并发度。节点即目录或者文件
* 读锁可以防止节点被删除、重命名或者被快照
* 写锁可以使得对同名节点的创建序列化
* 按照全局顺序获取锁，避免死锁
  * 按照 namespace tree 的层级，从顶层到底层依次获取
  * 同一层级，按照字典序依次获取

### 副本位置管理

### 创建、重建、再均衡

### 垃圾回收

> 设计要点

* 文件被删除后，不是立即回收物理存储，而是通过后台的周期性任务来回收
  * 后台的任务更具备可靠性，也不需要存储删除失败的信息
  * 编程上可以和定期的 namespace scan 以及 master 与 chunk server 间的 heartbeat 相结合
  * 可批量化
  * 可在闲时运行
* master 不认识的 chunk/replica 是垃圾
* master 中不被引用的 chunk handler 是垃圾

> 典型流程

* 被删除的文件实际上是被重命名，隐藏起来。在配置的时间（3 day）到期后，这些被隐藏的文件会被移除，此时 master 才会移除它们的 metadata
* 到期前文件可以被读取、被恢复
* 后台的周期性任务也会寻找引用计数为 0 的 chunk handler，并且删除它们的 metadata。此外，通过 heartbeat，chunk server 会上报它管理的 chunk 集合，master 会找出其中没有在 metadata 中被引用的 chunk，然后回复给 chunk server，chunk server 可以自行删除这些 chunk

> 存在的问题

* 物理存储很紧张的情况
  * 通过再次显式地删除来立即 reclaim 物理存储
  * 不同 namespace 下配置不同的策略

### stale 副本检测

## 容错和诊断

### high availability

* 快速恢复
* chunk replication
* master replication

### data integrity

### 诊断工具

* 诊断日志
  * 记录重要的事件
  * 记录所有的 rpc request 和 reply（不包括文件数据）
  * 异步地记录，最新的诊断日志在内存中，可以用于连续的在线监控