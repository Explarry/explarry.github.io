---
layout: post
title:  gfs
categories: gfs
---

# gfs

google file system

## 设计目标

* 要求组件不可用是正常情况，而不是异常情况
* 要求适用于存储大文件，数 G 的大文件
* 要求写操作以原子的追加数据为主，而不是修改现存的数据
* 要求应用层和存储层协同

## 基本假设

* 组件通常是便宜的商用产品
* 存储的通常是大文件，支持存储小文件，但是不会追求小文件的效率
* 应用读操作大部分是顺序读，支持随机读，但是不会追求随机读的效率
* 应用写操作大部分是追加，支持随机写，但是不会追求随机写的效率
* 支持并发的原子性追加，减小原子性带来的 overhead 是优化的重点
* 应用对吞吐量的要求很高，但对响应的及时性有一定的容忍度

## 接口

* create
* delete
* open
* close
* read
* write

* snapshot
* record append

## 架构

* 1 master + n chunkserver
* n client

> master 的目的：能够基于全局信息来解决存储在哪和副本怎样管理这两个的复杂问题

> master 的 metadata：命名空间，访问控制，文件到 chunk 的映射，chunk 的位置，lease 的管理，垃圾回收，chunk 的迁移

> 文件在存储时被划分成大小固定的 chunk，默认副本数为 3，不同的 namespace 可以设置不同的副本数

> chunk server 和 client 都不会缓存文件数据。chunk server 利用操作系统 buffer，client 缓存对于大文件收效甚微

* 控制面
* 数据面

> master 只负责全局的控制，master 不参与任何文件数据的传输
> client 和 chunk server 之间进行文件数据的读写

## master metadata

### master 内存中的数据

* 放到内存中，方便定期扫描、变动
* 每 64M 的 chunk 对应小于 64 bytes 的 metadata
* 每文件命名空间/文件名对于小于 64 bytes 的 metadata，这里使用了前缀压缩算法

### chunk location

* master 不会持久化 chunk location 数据，核心的逻辑在于 chunk server 对持久化后的 chunk 有最终话语权
* master 每次启动时向 chunk sever 查询各个 chunk location

### 操作记录

* 操作记录只有既记录了本地日志，也记录了几份远程日志，才会给 client 返回
* 为了吞吐量提高，使用了 batch 处理，即积攒几条记录一起 flush
* 使用 checkpoint 避免操作日志过大
* checkpoint 采用 compact b-tree like 数据结构，可以直接映射到内存使用
* 创建 checkpoint 需要花费少量时间，这要求 master 的内部状态采用的数据结构不会推迟新来的变更

## 一致性模型

### gfs guarantee

* 实现 gfs guarantee 凭借的是
  * namespace lock
  * global operation log
  * chunk version number，可以用于判断 chunk 是否 stale
  * stale chunk 永远不会在 master 询问 location 时被返回、永远不会参与后续的 mutation，相反地它会被垃圾回收

* 决定一致性状态的三个要素
  * 操作类型
  * 串行/并行
  * 成功/失败

||write|append|
|:-|:-|:-|
|串行成功|defined|`defined, interspersed with inconsistent`|
|并行成功|consistent, undefined|`defined, interspersed with inconsistent`|
|串行失败|inconsistent|inconsistent|
|并行失败|inconsistent|inconsistent|

* write: 在应用指定的 offset，写入数据

* append
  * offset 由 gfs 选择，在并发的情况下，保证原子性，保证至少写入一次
  * gfs 会返回这个 offset，这个 offset 代表一段 defined region 的开始，并且这个 definde region 必然包含被 append 的 record
  * 这也就意味着，gfs 可能会插入少量的 padding 或者 duplication
  * 返回 offset 保证了以 offset 开始的一个 region `defined`，但是少量的 padding 或者 duplication 又导致了 `interspersed with inconstent`

* 状态定义
  * consistent 状态：对于同一个 file region，无论从哪一个 replication 读取，所有 client 都能够看到同样的数据
  * defined 状态：前置要求 consistent，其次 clients will see what the mutation writes in its entirety（这里如何理解？）
    * 所有的 mutation 都被应用了，但是可能存在少量的 padding 或者 duplication

* 存在的问题
  * client 会缓存 chunk location，但是这个问题的影响不大，原因是：
    * client 缓存过期时间短
    * 大部分是 append 操作，即使读到 stale chunk，也只是会提前结束，缺失部分数据
    * 机审不是 append，也可以纠正，通过后续向 master 重新询问 chunk location
  * mutation 成功，但是后续 chunk server 可能发生数据损坏，但是这个问题影响不大，原因是：
    * 通过 checksum 识别损坏的数据，损坏的数据不会被返回，还可以从其它的 replication 读取
    * 会尽快从其它的有效 replication 恢复一个新的 chunk
    * 即使短时间内所有的 replication 都损坏了，应用也只会收到数据不可用的错误提示，而不是收到损坏的数据


### 对应用的隐含要求

* 尽量使用
  * append
  * checkpoint
  * writing self-validating
  * self-identifiying record

* reader 处理正确处理额外的 padding 和 duplication
  * 可以采用 checksum 达到这一目的
  * 使用唯一标志来过滤掉 duplication